[{"content":"While working on the Reddog Microservices Integration Application Sample, my objective was to demonstrate a development scenario that uses Bicep to integrate Azure API Management with Azure Container Apps.\nBicep is a domain-specific language (DSL) that uses declarative syntax to deploy Azure resources.\nAzure API Management is a managed service that allows you to manage services across hybrid and multi-cloud environments. API management acts as a facade to abstract the backend architecture, and it provides control and security for API observability and consumption for both internal and external users.\nAzure Container Apps is a fully managed, serverless container service used to build and deploy modern apps at scale. In this solution, you\u0026rsquo;re hosting microservices on Azure Container Apps and deploying them into a single Container App environment. This environment acts as a secure boundary around the system.\nCreating the APIM resource The first step was to create the main APIM Service resource:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @description(\u0026#39;The name of the API Management resource to be created.\u0026#39;) param apimName string @description(\u0026#39;The email address of the publisher of the APIM resource.\u0026#39;) @minLength(1) param publisherEmail string = \u0026#39;apim@contoso.com\u0026#39; @description(\u0026#39;Company name of the publisher of the APIM resource.\u0026#39;) @minLength(1) param publisherName string = \u0026#39;Contoso\u0026#39; @description(\u0026#39;The pricing tier of the APIM resource.\u0026#39;) param skuName string = \u0026#39;Developer\u0026#39; @description(\u0026#39;The instance size of the APIM resource.\u0026#39;) param capacity int = 1 @description(\u0026#39;Location for Azure resources.\u0026#39;) param location string = resourceGroup().location resource apimResource \u0026#39;Microsoft.ApiManagement/service@2020-12-01\u0026#39; = { name: apimName location: location sku: { capacity: capacity name: skuName } properties: { virtualNetworkType: \u0026#39;External\u0026#39; publisherEmail: publisherEmail publisherName: publisherName } } This will deploy a APIM instance. Of particular note here is virtualNetworkType declaration. This represents the type of VPN in which API Management service needs to be configured in:\nNone (Default Value) means the API Management service is not part of any Virtual Network, External means the API Management deployment is set up inside a Virtual Network having an Internet Facing Endpoint, and Internal means that API Management deployment is setup inside a Virtual Network having an Intranet Facing Endpoint only. NOTE: Please bear in mind that APIM takes more than an hour to provision.\nApplying a global policy Next step, I created a global policy definition file apimPolicies/global.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \u0026lt;policies\u0026gt; \u0026lt;inbound\u0026gt; \u0026lt;rate-limit-by-key calls=\u0026#34;1000\u0026#34; renewal-period=\u0026#34;60\u0026#34; increment-condition=\u0026#34;@(context.Response.StatusCode == 200)\u0026#34; counter-key=\u0026#34;@(context.Request.IpAddress)\u0026#34; remaining-calls-variable-name=\u0026#34;remainingCallsPerIP\u0026#34;/\u0026gt; \u0026lt;!-- Distributed tracing support by adding correlationId using COMB format--\u0026gt; \u0026lt;!-- NOTE: If COMB format is not needed, context.RequestId should be used as a value of correlation id. --\u0026gt; \u0026lt;!-- context.RequestId is unique for each request and is stored as part of gateway log records. --\u0026gt; \u0026lt;!-- https://learn.microsoft.com/en-us/azure/api-management/policies/add-correlation-id --\u0026gt; \u0026lt;set-header name=\u0026#34;correlationid\u0026#34; exists-action=\u0026#34;skip\u0026#34;\u0026gt; \u0026lt;value\u0026gt;@{ var guidBinary = new byte[16]; Array.Copy(Guid.NewGuid().ToByteArray(), 0, guidBinary, 0, 10); long time = DateTime.Now.Ticks; byte[] bytes = new byte[6]; unchecked { bytes[5] = (byte)(time \u0026gt;\u0026gt; 40); bytes[4] = (byte)(time \u0026gt;\u0026gt; 32); bytes[3] = (byte)(time \u0026gt;\u0026gt; 24); bytes[2] = (byte)(time \u0026gt;\u0026gt; 16); bytes[1] = (byte)(time \u0026gt;\u0026gt; 8); bytes[0] = (byte)(time); } Array.Copy(bytes, 0, guidBinary, 10, 6); return new Guid(guidBinary).ToString(); }\u0026lt;/value\u0026gt; \u0026lt;/set-header\u0026gt; \u0026lt;/inbound\u0026gt; \u0026lt;backend\u0026gt; \u0026lt;forward-request /\u0026gt; \u0026lt;/backend\u0026gt; \u0026lt;outbound\u0026gt; \u0026lt;redirect-content-urls /\u0026gt; \u0026lt;/outbound\u0026gt; \u0026lt;on-error /\u0026gt; \u0026lt;/policies\u0026gt; Then add the policy resource to the Bicep module:\n1 2 3 4 5 6 7 8 resource apimPolicy \u0026#39;Microsoft.ApiManagement/service/policies@2021-12-01-preview\u0026#39; = { name: \u0026#39;policy\u0026#39; parent: apimResource properties: { format: \u0026#39;rawxml\u0026#39; value: loadTextContent(\u0026#39;apimPolicies/global.xml\u0026#39;) } } The Policy Contract here is defined in xml format, and of particular note here is the loadTextContent function which loads the content of a specified file as a string.\nAdding a Backend API A backend (or API backend) in API Management is an HTTP service that implements your front-end API and its operations. Sometimes backend APIs are referred to simply as backends. API Management supports using other Azure resources as an API backend, such as Container Apps. A custom backend has several benefits, including:\nAbstracts information about the backend service, promoting reusability across APIs and improved governance. Easily used by configuring a transformation policy on an existing API. Takes advantage of API Management functionality to maintain secrets in Azure Key Vault if named values are configured for header or query parameter authentication. To create a Microsoft.ApiManagement/service/backends resource, I added the following Bicep to my template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 resource orderService \u0026#39;Microsoft.App/containerApps@2022-03-01\u0026#39; existing = { name: \u0026#39;order-service\u0026#39; } resource orderBackendResource \u0026#39;Microsoft.ApiManagement/service/backends@2021-12-01-preview\u0026#39; = { name: \u0026#39;order-service-backend\u0026#39; parent: apimResource dependsOn: [ orderService ] properties: { description: orderService.name url: \u0026#39;https://${orderService.properties.configuration.ingress.fqdn}\u0026#39; protocol: \u0026#39;http\u0026#39; resourceId: \u0026#39;${environment().resourceManager}${orderService.id}\u0026#39; } } Here we reference an existing Container App in our backend resource and use the ingress URL as the backend URL. The resourceId needs to be a URL for the deployment to work. Here we use the environment() function, returns properties for the current Azure environment, to get the management URL instead of hard-coding \u0026quot;https://management.azure.com/\u0026quot;.\nAdding a Fronted API API Management serves as mediation layer over the backend APIs. Frontend API is an API that is exposed to API consumers from API Management. You can customize the shape and behavior of a frontend API in API Management without making changes to the backend API(s) that it represents. Sometimes frontend APIs are referred to simply as APIs.\nTo create a Microsoft.ApiManagement/service/apis resource, I added the following Bicep to my template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 resource orderApiResource \u0026#39;Microsoft.ApiManagement/service/apis@2021-12-01-preview\u0026#39; = { parent: apimResource name: \u0026#39;order-service\u0026#39; properties: { displayName: \u0026#39;OrderService\u0026#39; subscriptionRequired: false path: \u0026#39;orders\u0026#39; protocols: [ \u0026#39;https\u0026#39; ] isCurrent: true } } subscriptionRequired: Specifies whether an API or Product subscription is required for accessing the API. path: Relative URL uniquely identifying this API and all of its resource paths within the API Management service instance. It is appended to the API endpoint base URL specified during the service instance creation to form a public URL for this API. Adding a Fronted API Policy A policy is a reusable and composable component, implementing some commonly used API-related functionality. API Management offers over 50 built-in policies that take care of critical but undifferentiated horizontal concerns - for example, request transformation, routing, security, protection, caching. The policies can be applied at various scopes, which determine the affected APIs or operations and dynamically configured using policy expressions. For more information, see Policies in Azure API Management.\nIn order to make the Fronted API policy reusable, I saved it in a XML file apimPolicies/api.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;policies\u0026gt; \u0026lt;inbound\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;set-backend-service id=\u0026#34;apim-generated-policy\u0026#34; backend-id=\u0026#34;{backendName}\u0026#34; /\u0026gt; \u0026lt;/inbound\u0026gt; \u0026lt;backend\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/backend\u0026gt; \u0026lt;outbound\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/outbound\u0026gt; \u0026lt;on-error\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/on-error\u0026gt; \u0026lt;/policies\u0026gt; This policy does one thing: Use the set-backend-service policy to redirect an incoming request to the specified backend for all operations in this Frontend API. For more information, see Set backend service.\nIn this case, backend-id is the Identifier (name) of the backend to route requests to, and since we are going to reuse this xml, we will add the {backendName} token to be replaced later.\nTo create a Microsoft.ApiManagement/service/apis/policies resource, I then added the following Bicep to my template:\n1 2 3 4 5 6 7 8 resource orderApiPolicy \u0026#39;Microsoft.ApiManagement/service/apis/policies@2021-12-01-preview\u0026#39; = { name: \u0026#39;policy\u0026#39; parent: orderApiResource properties: { value: replace(loadTextContent(\u0026#39;apimPolicies/api.xml\u0026#39;), \u0026#39;{backendName}\u0026#39;, orderBackendResource.name) format: \u0026#39;xml\u0026#39; } } Here we are basically leveraging a combination of the replace function and the loadTextContent function to read the XML file and replace the {backendName} token with the actual Backend API we want to route requests to.\nAdding a Fronted API Operation A frontend API in API Management can define multiple operations. An operation is a combination of an HTTP verb and a URL template uniquely resolvable within the frontend API. Often operations map one-to-one to backend API endpoints. For more information, see Mock API responses.\nTo create a Microsoft.ApiManagement/service/apis/operations resource, I added the following Bicep to my template:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 resource getOrdersOperationResource \u0026#39;Microsoft.ApiManagement/service/apis/operations@2021-12-01-preview\u0026#39; = { name: \u0026#39;get-orders-storeid\u0026#39; parent: orderApiResource properties: { displayName: \u0026#39;/orders/{storeId} - GET\u0026#39; method: \u0026#39;GET\u0026#39; urlTemplate: \u0026#39;/orders/{storeId}\u0026#39; templateParameters: [ { name: \u0026#39;storeId\u0026#39; type: \u0026#39;string\u0026#39; required: true } ] responses: [ { statusCode: 200 description: \u0026#39;Success\u0026#39; } ] } } Adding a Operation policy In order to make the Operation policy reusable, I saved it in a XML file apimPolicies/operation.xml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In order to make the Fronted API policy reusable, I saved it in a XML file `apimPolicies/api.xml`: ```xml \u0026lt;policies\u0026gt; \u0026lt;inbound\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;set-method\u0026gt;{method}\u0026lt;/set-method\u0026gt; \u0026lt;rewrite-uri id=\u0026#34;apim-generated-policy\u0026#34; template=\u0026#34;{template}\u0026#34; /\u0026gt; \u0026lt;set-header id=\u0026#34;apim-generated-policy\u0026#34; name=\u0026#34;Ocp-Apim-Subscription-Key\u0026#34; exists-action=\u0026#34;delete\u0026#34; /\u0026gt; \u0026lt;/inbound\u0026gt; \u0026lt;backend\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/backend\u0026gt; \u0026lt;outbound\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/outbound\u0026gt; \u0026lt;on-error\u0026gt; \u0026lt;base /\u0026gt; \u0026lt;/on-error\u0026gt; \u0026lt;/policies\u0026gt; This policy does three things:\nset-method: sets the HTTP request method for a request. See Set request method rewrite-url: converts a request URL from its public form to the form expected by the web service. See Rewrite URL set-header: assigns a value to an existing response and/or request header or adds/removes a new response and/or request header. See Set HTTP header Repeat Steps 3 - 7 are repeated for all Backend APIs, Frontend APIs, Policies, and operations that need to be defined on Azure API Management.\nHope this proves valuable to you, dear reader.\nReferences:\nBicep\nAzure API Management\nAzure Container Apps\nAzure API Management terminology\nBicep functions\nAPI Management policy reference\nDefine resources with Bicep, ARM templates, and Terraform AzAPI provider\n","date":"2022-09-28T15:58:11Z","image":"https://www.chingono.com/blog/2022/09/28/integrate-container-apps-api-management-bicep/cover_hu9184b176dbea12fd3656e15cabc7fb95_38934_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2022/09/28/integrate-container-apps-api-management-bicep/","title":"Integrate Container Apps With Api Management using Bicep"},{"content":"While working on the Reddog Microservices Integration Application Sample, I wanted to add support for Azure Functions to an existing Dev Container.\nThe first thing I tried was to add the following to my Dockerfile:\n1 2 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get -y install azure-functions-core-tools-4 Unfortunately, that resulted in errors when rebuilding the container. So after searching online, and reviewing documentation, I eventually landed on this RUN command:\n1 2 3 4 5 6 RUN curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor \u0026gt; microsoft.gpg \\ \u0026amp;\u0026amp; sudo mv microsoft.gpg /etc/apt/trusted.gpg.d/microsoft.gpg \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; sudo sh -c \u0026#39;echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/debian/$(lsb_release -rs | cut -d\u0026#39;.\u0026#39; -f 1)/prod $(lsb_release -cs) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/dotnetdev.list\u0026#39; \\ \u0026amp;\u0026amp; sudo apt-get update \\ \u0026amp;\u0026amp; apt-get -y install azure-functions-core-tools-4 This successfully rebuilt the container image and had the Azure Functions Core Tools installed.\nNext, I added the following extension to the devcontainer.json file:\n1 2 3 4 5 6 \u0026#34;extensions\u0026#34;: [ \u0026#34;ms-azuretools.vscode-azurefunctions\u0026#34;, \u0026#34;ms-vscode.azure-account\u0026#34;, \u0026#34;ms-azuretools.vscode-azureresourcegroups\u0026#34;, \u0026#34;azurite.azurite\u0026#34; ] With that, I had a working environment fully loaded with Azure Functions Core Tools and the necessary Visual Studio Code extensions installed. Hope that helps, dear future reader!\nReferences:\nUse a Docker container as a development environment with Visual Studio Code\nWork with Azure Functions Core Tools\nAzure Functions Core Tools\n","date":"2022-09-23T15:58:13Z","image":"https://www.chingono.com/blog/2022/09/23/adding-azure-functions-support-devcontainer/cover_hu64142fbbb0b213e6c590ff3fa2b810bc_35496_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2022/09/23/adding-azure-functions-support-devcontainer/","title":"Adding Azure Functions Support to a Devcontainer"},{"content":"Recently, I received a request from a client to demonstrate how to add Azure Cognitive Search functionality to a statically generated site built on Next.js.\nAzure Cognitive Search is a search-as-a-service cloud solution that gives developers APIs and tools for adding a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.\nNext.js is an open-source web development framework built on top of Node.js enabling React based web applications functionalities such as server-side rendering and generating static websites.\nFirst I followed the Next.js Getting Started guide and created a new site:\n1 2 3 mkdir -p ./nextjs-search/src cd ./nextjs-search/src npx create-next-app@latest Once the site was created, I ran the following command to launch the site:\n1 $ yarn dev Which produced the following output:\n1 2 3 4 5 yarn run v1.22.18 $ next dev ready - started server on 0.0.0.0:3000, url: http://localhost:3000 wait - compiling... event - compiled client and server successfully in 1058 ms (125 modules) At this stage, I could launch the site and click around. Nothing much to see yet.\nNext, I added the Ant Design React UI Library\n1 yarn add antd Then, added a search page in the following path: src/pages/search.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 import Head from \u0026#39;next/head\u0026#39; import Image from \u0026#39;next/image\u0026#39; import styles from \u0026#39;../styles/Search.module.css\u0026#39; import \u0026#39;antd/dist/antd.css\u0026#39; import { Input, Space, List } from \u0026#39;antd\u0026#39; import React, { useState } from \u0026#39;react\u0026#39;; const { Search } = Input; export default function Find() { const [ findings, setFindings ] = useState({ \u0026#34;count\u0026#34;: 0, \u0026#34;results\u0026#34;: [] }) const onSearch = async value =\u0026gt; { const res = await fetch(`/api/search?q=${value}`) const json = await res.json(); setFindings({ \u0026#34;count\u0026#34;: json.count || 0, \u0026#34;results\u0026#34;: json.results }); } return ( \u0026lt;div className={styles.container}\u0026gt; \u0026lt;Head\u0026gt; \u0026lt;title\u0026gt;Create Next App\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Generated by create next app\u0026#34; /\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;/favicon.ico\u0026#34; /\u0026gt; \u0026lt;/Head\u0026gt; \u0026lt;main className={styles.content}\u0026gt; \u0026lt;h1\u0026gt;Document Search\u0026lt;/h1\u0026gt; \u0026lt;Search placeholder=\u0026#34;input search text\u0026#34; allowClear enterButton=\u0026#34;Search\u0026#34; size=\u0026#34;large\u0026#34; onSearch={onSearch} /\u0026gt; \u0026lt;List className=\u0026#34;search-results\u0026#34; itemLayout=\u0026#34;horizontal\u0026#34; dataSource={findings.results} renderItem={item =\u0026gt; ( \u0026lt;List.Item actions={[\u0026lt;a key=\u0026#34;list-loadmore-more\u0026#34;\u0026gt;more\u0026lt;/a\u0026gt;]} \u0026gt; \u0026lt;List.Item.Meta avatar={\u0026lt;img src={item.picture} /\u0026gt;} title={\u0026lt;a href={\u0026#34;/api/document/\u0026#34;+item.name}\u0026gt;{item.name}\u0026lt;/a\u0026gt;} description={\u0026lt;\u0026gt;\u0026lt;span\u0026gt;\u0026lt;strong\u0026gt;Author:\u0026lt;/strong\u0026gt;\u0026amp;nbsp;{item.author}\u0026lt;/span\u0026gt;\u0026amp;nbsp;\u0026lt;span\u0026gt;\u0026lt;strong\u0026gt;Created:\u0026lt;/strong\u0026gt;\u0026amp;nbsp;{item.created}\u0026lt;/span\u0026gt;\u0026lt;/\u0026gt;} /\u0026gt; \u0026lt;div\u0026gt;content\u0026lt;/div\u0026gt; \u0026lt;/List.Item\u0026gt; )} /\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;footer className={styles.footer}\u0026gt; \u0026lt;a href=\u0026#34;https://vercel.com?utm_source=create-next-app\u0026amp;utm_medium=default-template\u0026amp;utm_campaign=create-next-app\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt; Powered by{\u0026#39; \u0026#39;} \u0026lt;span className={styles.logo}\u0026gt; \u0026lt;Image src=\u0026#34;/vercel.svg\u0026#34; alt=\u0026#34;Vercel Logo\u0026#34; width={72} height={16} /\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; ) } The key players here are the Search and List components which, as the names suggest, enable the user to type in a search phrase and render the list of search results.\nNext, I added the Azure Cognitive Search client library for JavaScript\n1 npm install @azure/search-documents Next, I added a local environment variable file src/.env.local:\n1 2 3 SEARCH_ENDPOINT=https://\u0026lt;search-service-name\u0026gt;.search.windows.net SEARCH_INDEX=\u0026lt;search-index-name\u0026gt; SEARCH_KEY=\u0026lt;search-service-search-key\u0026gt; Next.js comes with built-in support for environment variables, I am using to avoid accidentally leaking the search key and make it easier to deploy our application.\nThen added a API page to act as a proxy for the Azure Cognitive Search service. src/pages/api/search.js:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 const { SearchClient, AzureKeyCredential, } = require(\u0026#34;@azure/search-documents\u0026#34;); const client = new SearchClient( process.env.SEARCH_ENDPOINT, process.env.SEARCH_INDEX, new AzureKeyCredential(process.env.SEARCH_KEY) ); export default async function handler(request, response) { var url = new URL(request.url, `http://${request.rawHeaders[\u0026#39;Host\u0026#39;]}`); var query = url.searchParams.get(\u0026#39;q\u0026#39;); const search = await client.search(query); var results = new Array(); for await (const result of search.results) { results.push({ \u0026#34;name\u0026#34;: result.document.metadata_storage_name, \u0026#34;author\u0026#34;: result.document.metadata_author, \u0026#34;contentType\u0026#34;: result.document.metadata_storage_content_type, \u0026#34;size\u0026#34;: result.document.metadata_storage_size, \u0026#34;created\u0026#34;: result.document.metadata_creation_date, \u0026#34;lastModified\u0026#34;: result.document.metadata_storage_last_modified }); }; response.status(200).json({ \u0026#34;count\u0026#34;: search.count, \u0026#34;results\u0026#34;: results }) } NOTE:\nI opted not to use the client library directly on the search page because, we do not want our search key exposed in the browser. With that in place, our application is minimally functional. We can search documents and view the search results on the same page.\nIn a future post, I will show how I added features like Suggestions, AutoCompletion, and Facets to make the page more user friendly.\nHope you find this helpful, dear reader! The full source for this project can be found on my GitHub repo: https://github.com/achingono/nextjs-jam-search\nReferences:\nNext.js\nAzure Cognitive Search\nAzure Cognitive Search client library for JavaScript\nOverview of adding search to a website\nCreate and load Search Index with JavaScript\n","date":"2022-05-06T16:42:42Z","image":"https://www.chingono.com/blog/2022/05/06/integrate-nextjs-azure-cognitive-search/cover_hu0f9a87807e30aad5738be2ac078bda8f_9027_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2022/05/06/integrate-nextjs-azure-cognitive-search/","title":"HOW TO: Integrate Next.js with Azure Cognitive Search"},{"content":"I do not recall writing a poem before, let alone sharing it with a broad audience. So this is a big risk I\u0026rsquo;m taking all in the pretext of a \u0026ldquo;growth mindset\u0026rdquo;. However, I had a recent experience that put me in deep thought and sent me on a long walk. During my walk, I came up with the first three lines of this poem. I\u0026rsquo;ve been tweaking it a little at a time ever since, and it still feels imperfect. I\u0026rsquo;m sharing it here because it still needs work and I hope someone will take an interest in improving it.\nWithout further ado, here\u0026rsquo;s my very first poem:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 In the land of diversity and inclusion In a sea of different and tolerant people There is that one quiet voice Drowned in collective representations Denied of individual expression I hope my friend You will choose to be The one to lend A listening ear And seek to understand Before being understood And opt to disagree But still appreciate They have a story Yet to be told My hope is that at some point we will find a way to recognize, appreciate, and acknowledge the contradictions inherent in inclusion and use the resulting tensions as springboards to push us forward and closer not backwards and further apart.\nThank you for reading.\nReference:\nParadoxes of Inclusion: Understanding and Managing the Tensions of Diversity and Multiculturalism\n","date":"2022-04-29T15:18:21Z","image":"https://www.chingono.com/blog/2022/04/29/the-paradox-of-inclusion/cover_hu1c5d4a53976d25bec27adb079bde9970_696917_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2022/04/29/the-paradox-of-inclusion/","title":"The Paradox of Inclusion"},{"content":"Back in 2013, I created a nice little sample project demonstrating code generation among other things. Scott Hanselman had this to say about it:\nReally col http://t.co/dzTs3fnv \u0026quot;Northwind\u0026quot; demo app by @achingono on GitHub. Clever use of T4 and more. Starred! https://t.co/1GTE055f\n\u0026mdash; Scott Hanselman (@shanselman) January 9, 2013 I really enjoyed code generation with T4 in Visual Studio that I used it in all my projects. Four years later, I asked the following on Twitter:\n@mkristensen @VisualStudio is there an equivalent to T4Scaffolding for #dotnercore? I need to generate multiple files based on a template.\n\u0026mdash; Alfero Chingono (@achingono) December 1, 2017 Glenn von Breadmeister was kind enough to respond with:\nNot really no. At least not one that I am aware of. People like @anurse will tell you that you can use razor. But afaik there isnt anything quite like t4.\n\u0026mdash; Glenn von Breadmeister (@condrong) December 1, 2017 And Andrew Stanton-Nurse took it further by saying:\nRazor isn’t for generating anything other than HTML. My understanding is that T4Scaffolding generates code, which Razor is really bad at. I don’t think there’s a good replacement right now\n\u0026mdash; Andrew Stanton-Nurse (@analogrelay) December 5, 2017 I somewhat forgot about these conversations until recently when I started building another sample project and needed to generate some code for rapid development. Looking around, I couldn\u0026rsquo;t find something that suited my needs and besides, I had a lingering question: \u0026ldquo;Is razor really that bad at code generation?\u0026rdquo;. I figured maybe it\u0026rsquo;s worth a try.\nSo, I created a dotnet global tool for generating project files.\ndotnet-spawn is a roslyn-based code generator for dotnet that adds files to a project or solution. Code generation parameters can be supplied inline or from a response file. At this time dotnet-spawn is able to create a single file or multiple files of any extension based on a Razor file template.\nHow To Use Invoking the dotnet spawn command will generate project files based on the supplied parameters or response file. You can control how verbose the output will be by using the --verbosity option.\nCommand Options --project: The path to the project or solution file to analyze with Roslyn --template: The path to the template file used for code generation --output: The path to a file/folder. If the tool is generating a single file, then the path should be a file, otherwise, it should be a folder. --namespace: The namespace used for generated files, if applicable. This parameter is available in the Razor template and can be customized further. --generator: One of SingleFile or MultipleFile. As the names suggest, the SingleFile generator creates a single file and the MultipleFile generator creates multiple files. --match: The lambda expression representing the Roslyn symbols that should be used for code generation. --pattern: The lambda expression used by the MultipleFile generator to generate file names. --verbosity: Set the verbosity level. Allowed values are Debug, Info, and Quiet. The Help Option This tool provides an option to display a brief description of the available commands, options, and arguments. System.CommandLine automatically generates help output. For example:\n1 dotnet spawn --help produces the following output:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Description: Roslyn based code generator. Usage: dotnet-spawn [options] Options: -p, --project \u0026lt;project\u0026gt; (REQUIRED) Project file, Solution file or directory -t, --template \u0026lt;template\u0026gt; (REQUIRED) Specify the template used for the generated code file(s) -g, --generator \u0026lt;generator\u0026gt; (REQUIRED) Specify the generator used for the generated code file(s) -n, --namespace \u0026lt;namespace\u0026gt; (REQUIRED) Specify the namespace for the generated code file(s) -m, --match \u0026lt;match\u0026gt; Specify the lambda expression used for the nanes of the generated code file(s) -p, --pattern \u0026lt;pattern\u0026gt; Specify the lambda expression used for the nanes of the generated code file(s) -o, --output \u0026lt;output\u0026gt; (REQUIRED) Specify the file or folder used for the generated code file(s) [default: .] -v, --verbosity \u0026lt;Debug|Info|Quiet\u0026gt; Output verbosity [default: Info] -f, --force Force execution of command --version Show version information -?, -h, --help Show help and usage information Response File A response file is a file that contains a set of tokens for a command-line app. Response files are a feature of System.CommandLine that is useful in two scenarios:\nTo invoke a command-line app by specifying input that is longer than the character limit of the terminal. To invoke the same command repeatedly without retyping the whole line. To use a response file, enter the file name prefixed by an @ sign wherever in the line you want to insert commands, options, and arguments. The following lines are equivalent: 1 dotnet spawn @controllers.rsp Contents of controllers.rsp:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 --project ./src/Sample.sln --template ./src/Sample/Templates/Controller.cshtml --output ./src/Sample/Controllers --namespace Sample.Controllers --generator MultipleFile --match symbol =\u0026gt; symbol.IsReferenceType \u0026amp;\u0026amp; !(symbol.IsAbstract || symbol.IsNamespace || symbol.IsVirtual) --pattern model =\u0026gt; $\u0026#34;{model.Name}.cs\u0026#34; --verbosity Info How To Install You can install the latest build of the tool using the following command.\n1 dotnet tool install -g dotnet-spawn How To Uninstall You can uninstall the tool using the following command.\n1 dotnet tool uninstall -g dotnet-spawn ","date":"2022-04-10T14:31:58Z","image":"https://www.chingono.com/blog/2022/04/10/dotnet-spawn-global-tool/cover_hu66dd2538423f2960887539a786eeed4f_16336_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2022/04/10/dotnet-spawn-global-tool/","title":"Dotnet Spawn Global Tool"},{"content":"A favicon, which is an abbreviation of the word \u0026ldquo;favorite icon\u0026rdquo;, is a small icon that helps users visually identify and distinguish your website. Its main purpose is to help visitors locate your page easier when they have multiple tabs, bookmarks, shortcuts, and address bars open.\nWith so many platforms, devices, icon formats, and dimensions, it\u0026rsquo;s hard to know exactly what you should do to consistently present your website icon/brand everywhere. That\u0026rsquo;s the reason why there are multiple favicon generators available. Just a quick search for \u0026ldquo;favicon generator\u0026rdquo; will reveal a long list of websites that assist in creating favicons supported across a wide range of platforms, and devices.\nThe one I chose for this case was favicon.io. This process could\u0026rsquo;ve been accomplished with realfavicongenerator.net just as well.\nAll I had to do was take a cropped version of my profile picture. It\u0026rsquo;s important that the picture be square, by the way. Then uploaded it to favicon.io and out came a zip file with everything I needed. Once downloaded, I extracted the zip file and copied the contents to the \u0026ldquo;static\u0026rdquo; folder of my Hugo site:\nThe next step was to simply copy the HTML snippet generated on the download page and paste it into layouts/partials/head/custom.html\n1 2 3 4 \u0026lt;link rel=\u0026#34;apple-touch-icon\u0026#34; sizes=\u0026#34;180x180\u0026#34; href=\u0026#34;/apple-touch-icon.png\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;32x32\u0026#34; href=\u0026#34;/favicon-32x32.png\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/png\u0026#34; sizes=\u0026#34;16x16\u0026#34; href=\u0026#34;/favicon-16x16.png\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;manifest\u0026#34; href=\u0026#34;/site.webmanifest\u0026#34;\u0026gt; And that was it! I hope you find this post valuable, dear reader. All comments and feedback greatly appreciated.\nReferences:\nAdd favicon in config.toml · Issue #42\n","date":"2022-03-27T18:39:35Z","image":"https://www.chingono.com/blog/2022/03/27/adding-favicon-hugo-site/cover_hu2c8cb4ef37481515eef505f57d1e84e9_50792_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2022/03/27/adding-favicon-hugo-site/","title":"Add Favicon to a Hugo-Based Website"},{"content":"Now that I have custom tasks for building and serving my Hugo site in Visual Studio Code, I wanted to create posts quickly without having to remember the hugo command. So, this is what I did:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;New Post\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;title=\\\u0026#34;${input:title}\\\u0026#34; \u0026amp;\u0026amp; slug=\\\u0026#34;${title// /-}\\\u0026#34; \u0026amp;\u0026amp; hugo new content/post/${slug,,}/index.md --source ./src\u0026#34;, \u0026#34;problemMatcher\u0026#34;: [] } ], \u0026#34;inputs\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Enter the title of the new post\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;promptString\u0026#34; } ] } The Variable Substitution section of the Visual Studio docs provides information on getting input from the user.\nFirst, I defined the inputs by providing the id, description, and type. Then the most interesting part:\nThe command:\n1 title=\u0026#34;${input:title}\u0026#34; \u0026amp;\u0026amp; slug=\u0026#34;${title// /-}\u0026#34; \u0026amp;\u0026amp; hugo new content/post/${slug,,}/index.md --source ./src does three (3) things using bash:\nSave the input text into a variable named title\n1 title=\u0026#34;${input:title}\u0026#34; Replace spaces in the title variable with dashes and store the output in another variable named slug:\n1 slug=\u0026#34;${title// /-}\u0026#34; Execute hugo command to create the content file:\n1 hugo new content/post/${slug,,}/index.md --source ./src This last command converts the contents of the slug variable to lowercase. Pretty neat!\nSo far I\u0026rsquo;m pretty pleased with my experience with the Hugo static site generator.\n","date":"2022-03-17T18:39:12Z","image":"https://www.chingono.com/blog/2022/03/17/defining-task-quickly-creating-hugo-posts-vscode/cover_hub8e99cfaeac9e6974b03f6aa110414f0_82550_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2022/03/17/defining-task-quickly-creating-hugo-posts-vscode/","title":"Defining A Task for Quickly Creating Hugo Posts In Visual Studio Code"},{"content":"Now that I can run the site locally, I needed a quick way to run the hugo command for creating new blog posts. After quick search on the internet, I found the article Tasks in Visual Studio Code very helpful. In short, I had to define the build task this way:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;hugo --source ./src\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } } ] } This task executes the hugo command which generates your website to the public/ directory by default and makes it ready to be deployed to your web server. The --source argument ensures the correct folder is built. In addition, setting \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; and \u0026quot;isDefault\u0026quot;: true ensures that this task is executed with the Ctrl+Shift+B keyboard shortcut.\nNext, I added another task to run the site locally:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \u0026#34;version\u0026#34;: \u0026#34;2.0.0\u0026#34;, \u0026#34;tasks\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;Build\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;hugo --source ./src\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;isDefault\u0026#34;: true } }, { \u0026#34;label\u0026#34;: \u0026#34;Serve\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;hugo server -D --source ./src\u0026#34;, \u0026#34;group\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;build\u0026#34; }, \u0026#34;isBackground\u0026#34;: true, \u0026#34;problemMatcher\u0026#34;: [] } ] } This task executes the hugo server command. The -D flag ensures that we can preview content in draft mode, and again, the --source argument ensures the correct folder is served. It\u0026rsquo;s important to note that the Serve does not have \u0026quot;isDefault\u0026quot;: true since we do not want the two tasks to conflict when using the Ctrl+Shift+B keyboard shortcut.\nSo far I\u0026rsquo;m pretty pleased with my experience with the Hugo static site generator.\n","date":"2022-03-16T20:44:03Z","image":"https://www.chingono.com/blog/2022/03/16/defining-tasks-quickly-building-serving-hugo-site/cover_hu853181193b62abd8fd8dded5a69822fd_43411_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2022/03/16/defining-tasks-quickly-building-serving-hugo-site/","title":"Defining Tasks for Quickly Building and Serving a Hugo Site"},{"content":" Clone repo in Codespaces\nAdd Hugo devcontainer\nClick the \u0026ldquo;Codespaces\u0026rdquo; button in the bottom-left corner of Visual Studio Code Click \u0026ldquo;Add Development Container Configuration Files\u0026rdquo; Click \u0026ldquo;Show All Definitions\u0026rdquo; Click \u0026ldquo;Hugo (Community)\u0026rdquo; Click \u0026ldquo;OK\u0026rdquo; Create new hugo site:\n1 \u0026gt;hugo new site blog Try to create new hugo module https://docs.stack.jimmycai.com/getting-started\n1 2 \u0026gt;hugo mod init github.com/achingono/achingono.github.io Error: failed to init modules: binary with name \u0026#34;go\u0026#34; not found Add go binary to devcontainer docker file\n1 2 3 4 5 6 7 8 9 10 # GO version ARG GO_VERSION=18 #Download Go RUN wget https://golang.org/dl/go1.${GO_VERSION}.linux-amd64.tar.gz \u0026amp;\u0026amp; \\ tar -C /usr/local -xzf go1.${GO_VERSION}.linux-amd64.tar.gz \u0026amp;\u0026amp; \\ rm go1.${GO_VERSION}.linux-amd64.tar.gz ENV GOPATH /go ENV PATH $GOPATH/bin:/usr/local/go/bin:$PATH Check go version\n1 2 \u0026gt;go version go version go1.18 linux/amd64 Turn new site into a Hugo module:\n1 2 3 4 \u0026gt;hugo mod init github.com/achingono/achingono.github.io go: creating new go.mod: module github.com/achingono/achingono.github.io go: to add module requirements and sums: go mod tidy Declare the hugo-theme-stack module as a dependency of your site:\n1 2 3 4 \u0026gt;hugo mod get github.com/CaiJimmy/hugo-theme-stack/v3 go: downloading github.com/CaiJimmy/hugo-theme-stack/v3 v3.10.0 go: downloading github.com/CaiJimmy/hugo-theme-stack v2.6.0+incompatible go: added github.com/CaiJimmy/hugo-theme-stack/v3 v3.10.0 Grab config file from example site\n1 \u0026gt;wget https://raw.githubusercontent.com/CaiJimmy/hugo-theme-stack/master/exampleSite/config.yaml Update config.yaml file\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 baseurl: https://achingono.github.io languageCode: en-us theme: github.com/CaiJimmy/hugo-theme-stack/v3 paginate: 5 title: Alfero Chingono languages: en: languageName: English title: Alfero Chingono weight: 1 # Change it to your Disqus shortname before using disqusShortname: achingono # GA Tracking ID googleAnalytics: # Theme i18n support # Available values: en, fr, id, ja, ko, pt-br, zh-cn, zh-tw, es, de, nl, it, th, el, uk, ar DefaultContentLanguage: en # Set hasCJKLanguage to true if DefaultContentLanguage is in [zh-cn ja ko] # This will make .Summary and .WordCount behave correctly for CJK languages. hasCJKLanguage: false permalinks: post: /blog/:slug/ page: /:slug/ ... module: # uncomment line below for temporary local development of module # replacements: \u0026#34;github.com/CaiJimmy/hugo-theme-stack/v3 -\u0026gt; ../../hugo-theme-stack\u0026#34; imports: - path: github.com/CaiJimmy/hugo-theme-stack/v3 disable: false Delete config.toml file Create new post Run hugo server -D Browse to http://127.0.0.1:1313 ","date":"2022-03-15T00:00:00Z","image":"https://www.chingono.com/blog/2022/03/15/hugo-blog-powered-by-github-pages-codespaces/cover_hu5e91b4f7e7a4ba6ba63ef7778b07ef87_84336_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2022/03/15/hugo-blog-powered-by-github-pages-codespaces/","title":"Hugo blog powered by GitHub Pages and GitHub Codespaces"},{"content":"A Distribution Certificate is used to identify a developer for the purpose of installing and testing apps on iOS devices. Certificates can be obtained from Apple\u0026rsquo;s iOS Provisioning Portal and a Certificate Signing Request (CSR) file needs to be generated first.\nThe process for generating a CSR differs depending on your choice of operating system. In this post, I\u0026rsquo;ll cover how to generate Certificate Signing Request (CSR) file on Windows as well as how to use the generated certificate and provisioning profile to build and sign an iOS application in Azure DevOps.\nFirst open a PowerShell window, then run the following command to create a private key:\n1 openssl genrsa -out appstore-distribution.key 2048 Next, run the following command to generate the Certificate Signing Request (CSR) file using the previously created private key:\n1 openssl req -new -key appstore-distribution.key -out appstore-distribution.csr -subj \u0026#34;/emailAddress=devops@example.com, CN=Example, C=CA\u0026#34; Next, go to the Apple Developer Portal and click the \u0026ldquo;+\u0026rdquo; button next to \u0026ldquo;Certificates\u0026rdquo;:\nChoose the type of certificate you want to create, then click \u0026ldquo;Continue\u0026rdquo;\nUpload the Certificate Signing Request (CSR) file generated earlier and click \u0026ldquo;Continue\u0026rdquo;:\nA certificate will be generated and will be available for download. click \u0026ldquo;Download the Certificate\u0026rdquo;.\nIn order to sign the application in Azure DevOps, we need the certificate in .p12 format. So to convert the .cer file to .p12, execute the following commands:\n1 openssl x509 -in appstore-distribution.cer -inform DER -out appstore-distribution.pem -outform PEM followed by:\n1 openssl pkcs12 -export -out appstore-distribution.p12 -inkey appstore-distribution.key -in appstore-distribution.pem This will prompt for a password. Please provide a password and store it in a safe place. This password will be needed when signing the application.\nNext, we need to generate a provisioning profile by going back to the Apple Developer Portal and clicking \u0026ldquo;+\u0026rdquo; next to \u0026ldquo;Profiles\u0026rdquo;:\nChoose the type of Provisioning Profile you want to create, then click \u0026ldquo;Continue\u0026rdquo;\nSelect the App ID you would like associated with the provisioning profile, then click \u0026ldquo;Continue\u0026rdquo;.\nSelect the certificate we created earlier, then click \u0026ldquo;Continue\u0026rdquo;.\nGive the provisioning profile a name, then click \u0026ldquo;Generate\u0026rdquo;. You will be able to download the provisioning profile after it is generated.\nUpload the P12 certificate and provisioning profile to Azure DevOps Secure Files Library. During upload, your certificate will be encrypted and securely stored.\nCreate a new pipeline, or go to the pre-existing pipeline if one already exists.\nGo to the Variables tab and add the following variables:\nCertificateFile: Set the value to appstore-distribution.p12 (or whatever name you gave to your P12 certificate file in step #13). CertificatePassword: Set the value to the password you set in step #7 above. Be sure check \u0026ldquo;Keep this value secret\u0026rdquo;. This will secure your password and obscure it in logs. ProvisioningFile: Set the value to appstore-distribution.mobileprovision (or whatever name you gave your provisioning file in step #13). Update the azure-pipelines.yml using the following example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 name: $(Build.BuildId) trigger: - main - dev pool: vmImage: \u0026#39;macOS-latest\u0026#39; variables: - name: configuration value: Release - name: sdk value: iphoneos - name: scheme value: Example - name: workspace value: Example.xcworkspace - name: plistFile value: Info.plist steps: - task: Cache@2 inputs: key: \u0026#39;pods | \u0026#34;$(Agent.OS)\u0026#34; | Podfile.lock\u0026#39; path: \u0026#39;Pods\u0026#39; cacheHitVar: \u0026#39;PODS_CACHE_RESTORED\u0026#39; - task: CocoaPods@0 displayName: \u0026#39;pod install using the CocoaPods task with defaults\u0026#39; inputs: forceRepoUpdate: true condition: ne(variables.PODS_CACHE_RESTORED, \u0026#39;true\u0026#39;) - task: InstallAppleCertificate@2 inputs: certSecureFile: \u0026#39;$(certificateFile)\u0026#39; certPwd: \u0026#39;$(CertificatePassword)\u0026#39; keychain: \u0026#39;temp\u0026#39; deleteCert: true - task: InstallAppleProvisioningProfile@1 inputs: provisioningProfileLocation: \u0026#39;secureFiles\u0026#39; provProfileSecureFile: \u0026#39;$(provisioningFile)\u0026#39; - task: CmdLine@2 displayName: \u0026#39;Set Build Number\u0026#39; inputs: script: \u0026#39;/usr/libexec/PlistBuddy -c \u0026#34;Set :CFBundleVersion $(Build.BuildId)\u0026#34; $(Build.SourcesDirectory)/$(scheme)/$(plistFile)\u0026#39; - task: Xcode@5 inputs: actions: \u0026#39;build\u0026#39; configuration: \u0026#39;$(configuration)\u0026#39; sdk: \u0026#39;$(sdk)\u0026#39; xcWorkspacePath: \u0026#39;$(workspace)\u0026#39; scheme: \u0026#39;$(scheme)\u0026#39; packageApp: true signingOption: \u0026#39;manual\u0026#39; signingIdentity: \u0026#39;$(APPLE_CERTIFICATE_SIGNING_IDENTITY)\u0026#39; provisioningProfileUuid: \u0026#39;$(APPLE_PROV_PROFILE_UUID)\u0026#39; - task: CopyFiles@2 inputs: contents: \u0026#39;**/*.ipa\u0026#39; targetFolder: \u0026#39;$(build.artifactStagingDirectory)\u0026#39; - task: PublishBuildArtifacts@1 displayName: \u0026#39;Publish artifact\u0026#39; condition: succeededOrFailed() References:\nConvert .cer to .p12\nBuild, test, and deploy Xcode apps\nSign your mobile app\n","date":"2021-08-30T22:46:59Z","image":"https://www.chingono.com/blog/2021/08/30/build-sign-ios-application-using-azure-devops/cover_hu1baf13393af83955e60d8ead591edd95_56695_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2021/08/30/build-sign-ios-application-using-azure-devops/","title":"How to Build and Sign iOs Application Using Azure DevOps"},{"content":"This is a continuation from my previous posts; Dockerizing Blazor Wasm Application\nThe previous setup was a follows: One drawback of this setup was that both the SPA service and the API service had to expose ports in order to be accessible and the ports had to be different. I didn\u0026rsquo;t like that very much and for my learning purposes, I set out to figure out how to accomplish the following setup:\nThe first task was to figure out how to configure NGINX to forward requests to multiple back-end services on the same port. After much research, this is the nginx.conf I eventually came up with:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 worker_processes 1; events { worker_connections 1024; } http { sendfile on; upstream spa-service { server SPA_SERVICE; } upstream api-service { server API_SERVICE; } # https://www.bogotobogo.com/DevOps/Docker/Docker-Compose-Nginx-Reverse-Proxy-Multiple-Containers.php proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; server { listen PORT; location / { proxy_pass http://spa-service/; proxy_redirect off; } # https://stackoverflow.com/a/53354944 location ~* /Api/v1\\.0/(.*) { # https://stackoverflow.com/a/8130872 proxy_pass http://api-service/Api/v1.0/$1$is_args$args; proxy_redirect off; # https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-nginx?view=aspnetcore-5.0 proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection keep-alive; proxy_cache_bypass $http_upgrade; proxy_set_header X-Forwarded-Proto $scheme; } } } The tricky part was to ensure that only traffic intended for the SPA service was routed there. Somehow, I could not get it to work using just the path directive. I had to resort to regex to get it to work.\nNext, I created a configure-environment.sh script very similar to the previous article:\n1 2 3 4 5 6 7 #!/bin/sh # replace the placeholders in the nginx congfiguration file # https://stackoverflow.com/a/23134318 sed -i -e \u0026#34;s|API_SERVICE|${API_SERVICE}|g\u0026#34; /etc/nginx/nginx.conf sed -i -e \u0026#34;s|SPA_SERVICE|${SPA_SERVICE}|g\u0026#34; /etc/nginx/nginx.conf sed -i -e \u0026#39;s/PORT/\u0026#39;\u0026#34;${PORT}\u0026#34;\u0026#39;/g\u0026#39; /etc/nginx/nginx.conf Details of the configuration settings are clearly outlined in the referenced articles.\nNext, the Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM nginx:alpine AS runtime # copy startup commands COPY configure-environment.sh /docker-entrypoint.d/ RUN chmod +x /docker-entrypoint.d/configure-environment.sh # https://www.bogotobogo.com/DevOps/Docker/Docker-Compose-Nginx-Reverse-Proxy-Multiple-Containers.php COPY nginx.conf /etc/nginx/nginx.conf ENV SPA_SERVICE=spa:8080 ENV API_SERVICE=api:8080 ENV PORT=8080 EXPOSE 8080 WORKDIR /home/site/wwwroot Finally, the docker-compose.yml to tie it all together:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # https://docs.docker.com/compose/compose-file/compose-file-v3/ version: \u0026#34;3\u0026#34; services: proxy: build: context: . dockerfile: Dockerfile image: business/proxy:latest container_name: business-proxy environment: - SPA_SERVICE=spa:8080 - API_SERVICE=api:8080 # https://docs.docker.com/compose/startup-order/ depends_on: - spa - api restart: always ports: - 80:8080 References:\nDOCKER COMPOSE : NGINX REVERSE PROXY WITH MULTIPLE CONTAINERS\nHost ASP.NET Core on Linux with Nginx\n\u0026ldquo;proxy_pass\u0026rdquo; cannot have URI part in location given by regular expression\nHow can query string parameters be forwarded through a proxy_pass with nginx?\nEnvironment variable substitution in sed\n","date":"2021-05-25T21:43:26Z","permalink":"https://www.chingono.com/blog/2021/05/25/using-nginx-reverse-proxy-for-local-microservice-development/","title":"Using Nginx Reverse Proxy for Local Microservice Development"},{"content":"This is a continuation from my previous posts; Dockerizing Blazor Wasm Application and Waiting for Docker Service Container Port to Be Ready.\nOne of the main reasons I needed my application container to wait for the database container to be ready was because I needed to initialize and seed the database before launching the application. This created a significant, and somewhat unacceptable, delay in container startup which impacted local development experience and automated UI tests.\nI went on a quest to search for a solution to this problem and found out that Database engines such as MySQL support the ability to automatically restore database backups when creating a docker container. Surely, Microsoft SQL Server would have the same feature, right? Apparently, not. So I had no choice but to roll out my own solution, right? Right!\nNow that we agree on the legitimacy of my quest, here are the changes I made to my project.\nFirst, I created a entrypoint.sh bash script with the following contents:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #!/bin/bash # Adapted from: github.com/microsoft/mssql-docker/issues/11 # Launch MSSQL and send to background /opt/mssql/bin/sqlservr \u0026amp; pid=$! # Wait for it to be available echo \u0026#34;Waiting for MS SQL to be available ⏳\u0026#34; /opt/mssql-tools/bin/sqlcmd -l 30 -S localhost -h-1 -V1 -U sa -P $SA_PASSWORD -Q \u0026#34;SET NOCOUNT ON SELECT \\\u0026#34;YAY WE ARE UP\\\u0026#34; , @@servername\u0026#34; is_up=$? while [ $is_up -ne 0 ] ; do echo -e $(date) /opt/mssql-tools/bin/sqlcmd -l 30 -S localhost -h-1 -V1 -U sa -P $SA_PASSWORD -Q \u0026#34;SET NOCOUNT ON SELECT \\\u0026#34;YAY WE ARE UP\\\u0026#34; , @@servername\u0026#34; is_up=$? sleep 5 done LOG_FILE=output.log # check flag so that this is only done once on creation, # and not every time the container runs if [ ! -f \u0026#34;${SCRIPTS_PATH}/${LOG_FILE}\u0026#34; ]; then # Run every bash script in /var/opt/mssql/scripts # https://stackoverflow.com/a/49383879 for file in \u0026#34;${SCRIPTS_PATH}/\u0026#34;*.sh; do echo \u0026#34;Executing: ${file}\u0026#34; \u0026gt;\u0026gt; \u0026#34;${SCRIPTS_PATH}/${LOG_FILE}\u0026#34; if [ -x \u0026#34;$file\u0026#34; ]; then echo \u0026#34;Executing $file\u0026#34;; \u0026#34;$file\u0026#34; else # warn on shell scripts without exec bit echo \u0026#34;Ignoring $file, not executable.\u0026#34;; fi done # https://stackoverflow.com/a/49383879 for file in \u0026#34;${SCRIPTS_PATH}/\u0026#34;*.sql; do echo \u0026#34;Executing: ${file}\u0026#34; \u0026gt;\u0026gt; \u0026#34;${SCRIPTS_PATH}/${LOG_FILE}\u0026#34; if test -f \u0026#34;$file\u0026#34;; then echo \u0026#34;Executing $file\u0026#34;; /opt/mssql-tools/bin/sqlcmd -U sa -P $SA_PASSWORD -l 30 -e -i $file fi done echo \u0026#34;All scripts have been executed.\u0026#34; fi echo \u0026#34;Waiting for MS SQL(pid $pid) to terminate.\u0026#34; # trap SIGTERM and send same to sqlservr process for clean shutdown trap \u0026#34;kill -15 $pid\u0026#34; SIGTERM # Wait on the sqlserver process wait $pid The script above does four main things:\nStart MSSQL engine in the background Wait for the engine to be ready for connections Execute scripts in a specified folder Cleanly shutdown the MSSQL process when the SIGTERM signal is received. You may also notice that there are two loops through the files in the scripts folder. The first loop iterates over bash scripts while the second iterates over SQL scripts.\nNext, I created a script file that restores the database:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/bash if [ \u0026#34;${DATABASE}\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;\u0026#39;DATABASE\u0026#39; environment variable not set.\u0026#34; exit 0 fi BACKUP_FILE=/var/opt/mssql/backup/${DATABASE}.bak DATA_FILE=/var/opt/mssql/data/${DATABASE}.mdf LOG_FILE=/var/opt/mssql/data/${DATABASE}_log.ldf if test -f \u0026#34;${BACKUP_FILE}\u0026#34;; then echo \u0026#34;${BACKUP_FILE} found.\u0026#34; else echo \u0026#34;${BACKUP_FILE} not found.\u0026#34; exit 0 fi echo \u0026#34;Restoring database ${DATABASE}\u0026#34; /opt/mssql-tools/bin/sqlcmd \\ -S localhost -U sa -P ${SA_PASSWORD} \\ -Q \u0026#39;RESTORE DATABASE \u0026#39;${DATABASE}\u0026#39; FROM DISK = \u0026#34;\u0026#39;${BACKUP_FILE}\u0026#39;\u0026#34; WITH MOVE \u0026#34;\u0026#39;${DATABASE}\u0026#39;\u0026#34; TO \u0026#34;\u0026#39;${DATA_FILE}\u0026#39;\u0026#34;, MOVE \u0026#34;\u0026#39;${DATABASE}\u0026#39;_Log\u0026#34; TO \u0026#34;\u0026#39;${LOG_FILE}\u0026#39;\u0026#34;\u0026#39; All this script does is check for a DATABASE environment variable and by convention, a backup file with a matching name in a specified folder. If both exist, then it executes a RESTORE DATABASE command.\nThen, I created a Dockerfile with the following contents:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ARG DATABASE=Business ARG SCRIPTS_PATH=/var/opt/mssql/scripts FROM mcr.microsoft.com/mssql/server:2019-latest ARG DATABASE ARG SCRIPTS_PATH COPY --chown=mssql:mssql entrypoint.sh /bin RUN chmod +x /bin/entrypoint.sh RUN mkdir -p ${SCRIPTS_PATH} COPY --chown=mssql:mssql restore-database.sh ${SCRIPTS_PATH} RUN chmod +x ${SCRIPTS_PATH}/restore-database.sh RUN mkdir -p /var/opt/mssql/backup COPY --chown=mssql:mssql database.bak /var/opt/mssql/backup RUN mv /var/opt/mssql/backup/database.bak /var/opt/mssql/backup/${DATABASE}.bak ENV DATABASE=${DATABASE} ENV SCRIPTS_PATH=${SCRIPTS_PATH} # https://docs.docker.com/compose/aspnet-mssql-compose/ ENTRYPOINT [\u0026#34;/bin/entrypoint.sh\u0026#34;] This Dockerfile basically copies three files into the base MSSQL container:\nentrypoint.sh restore-database.sh database.bak Once those three files are copied to the right folders and made executable, the magic happens when the container is created.\nThe last step was to update the docker-compose.yml file with the following changes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 data: - image: mcr.microsoft.com/mssql/server:2019-latest + build: + context: . + dockerfile: Dockerfile + image: business/data:latest + container_name: business-data environment: - SA_PASSWORD=${SQL_PASSWORD} - ACCEPT_EULA=Y + - DATABASE=${SQL_DATABASE} restart: unless-stopped ports: - 1433:1433 With those changes in place, the database container creation process was multiple times faster than before and my automated UI tests were happier for it.\nReferences:\nRestore a SQL Server database in a Linux Docker container\nBASH - FOR loop using LS and wildcard\n","date":"2021-04-21T10:20:50Z","image":"https://www.chingono.com/blog/2021/04/21/restore-database-on-container-start-up/cover_hua6d920ccffbe7712bb5e078193bdddf7_60920_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2021/04/21/restore-database-on-container-start-up/","title":"Restore Database on Container Start Up"},{"content":"This is a continuation of my previous post; Waiting for Docker Service Container Port to Be Ready. After reading this article, I decided to build up on that codebase as a learning opportunity and add a Blazor WASM front-end to the .NET Core API already built.\nI will not repeat the process details as that is nicely outlined in the referenced article. What I will do here is share my version of the Dockerfile and the associated script and configuration file.\nSo, here\u0026rsquo;s my Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # [Choice] .NET Core version: 5.0, 3.1, 2.1 ARG VARIANT=5.0 ARG PROJECT=Business.Spa ARG CONFIG=Release # create build image from base SDK image FROM mcr.microsoft.com/dotnet/sdk:${VARIANT} as build # https://github.com/moby/moby/issues/37345#issuecomment-400245466 ARG PROJECT # Copy all the csproj files and restore to cache the layer for faster builds # https://github.com/dotnet/dotnet-docker/issues/1697#issuecomment-589420446 COPY src/Business.Spa/Business.Spa.csproj ./src/Business.Spa/ # restore nuget packages WORKDIR /src RUN dotnet restore # run publish command FROM build as publish # https://github.com/moby/moby/issues/37345#issuecomment-400245466 ARG PROJECT ARG CONFIG # copy source files COPY src/ /src/ # run the publish command WORKDIR /src/${PROJECT} RUN dotnet publish -c ${CONFIG} -o /${CONFIG} --no-restore # create release image from base runtime image FROM nginx:alpine AS runtime ARG CONFIG RUN apk update \\ \u0026amp;\u0026amp; apk add --no-cache openssh # copy startup commands COPY ./configure-environment.sh /docker-entrypoint.d/ RUN chmod +x /docker-entrypoint.d/configure-environment.sh # copy the nginx configuration file # https://www.c-sharpcorner.com/article/dockerizing-blazor-wasm-application/ COPY ./nginx.conf /etc/nginx/nginx.conf ENV BLAZOR_ENVIRONMENT=Staging ENV REST_URL=http://localhost:8081 ENV PORT=8080 EXPOSE 8080 WORKDIR /home/site/wwwroot COPY --from=publish /${CONFIG}/wwwroot . You will notice that my Dockerfile does not have a CMD or ENTRYPOINT declaration. That is because the base image I\u0026rsquo;m using to host my application nginx:alpine already has a very nice feature where it automatically runs all scripts in the /docker-entrypoint.d/ folder and executes them before starting up nginx.\nThis allowed me to copy my configure-environment.sh to the /docker-entrypoint.d/ folder, make it executable and let the magic happen.\nHere\u0026rsquo;s my configure-environment.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/sh # The script replaces the Api.BaseAddress in the configuration file # with environment values set for the container # replace the placeholder in the blazor congfiguration file # https://stackoverflow.com/a/23134318 sed -i -e \u0026#34;s|REST_URL|${REST_URL}|g\u0026#34; \u0026#34;/home/site/wwwroot/appsettings.${BLAZOR_ENVIRONMENT}.json\u0026#34; # replace the placeholders in the nginx congfiguration file # https://github.com/dotnet/aspnetcore/issues/25152 sed -i -e \u0026#39;s/BLAZOR_ENVIRONMENT/\u0026#39;\u0026#34;${BLAZOR_ENVIRONMENT}\u0026#34;\u0026#39;/g\u0026#39; /etc/nginx/nginx.conf sed -i -e \u0026#39;s/PORT/\u0026#39;\u0026#34;${PORT}\u0026#34;\u0026#39;/g\u0026#39; /etc/nginx/nginx.conf And finally, here\u0026rsquo;s my nginx configuration file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 events { } http { include mime.types; types { application/wasm wasm; } server { # this will be replaced by the sed command in the configure-environment.sh script listen PORT; index index.html; # This will add the environment http header to the responses # https://github.com/dotnet/aspnetcore/issues/25152 add_header Blazor-Environment BLAZOR_ENVIRONMENT; location / { root /home/site/wwwroot; try_files $uri $uri/ /index.html =404; } } } Credits:\nDockerizing Blazor WASM Application\ndocker-nginx/docker-entrypoint.sh\nlinux - Environment variable substitution in sed\nASP.NET Core Blazor environments\nBlazor WASM not loading appsettings.{environment}.json in Azure App Services\n","date":"2021-04-07T18:39:49Z","image":"https://www.chingono.com/blog/2021/04/07/dockerizing-blazor-wasm-application/cover_hu54647e4733936e0a413cf65fbe4dc5e0_77413_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2021/04/07/dockerizing-blazor-wasm-application/","title":"Dockerizing Blazor Wasm Application"},{"content":"In my previous post; Combining ENTRYPOINT and CMD in a Dockerfile, I showed how I added reusability to my Dockerfile by combining ENTRYPOINT and CMD.\nTo take things further, I didn\u0026rsquo;t like how I still had to supply arguments to entrypoint.sh when those arguments were already available in the container as environment variables, so I eventually modified my Dockerfile this way:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # [Choice] .NET Core version: 5.0, 3.1, 2.1 ARG VARIANT=3.1 ARG DB_SERVICE=db ARG DB_SERVICE_PORT=1433 ARG PROJECT=Business.Web # create a base runtime image with node FROM mcr.microsoft.com/dotnet/core/aspnet:${VARIANT} AS runtime EXPOSE 80 EXPOSE 443 RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # create a base SDK image with node FROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT} as sdk RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # copy source files FROM sdk as build COPY src/ ./src # restore nuget packages WORKDIR /src RUN dotnet restore # run publish command FROM build as publish RUN dotnet publish -c Release -o /release --no-restore # create release image from base runtime image FROM runtime AS release # https://github.com/moby/moby/issues/37345#issuecomment-400245466 ARG PROJECT ENV ASSEMBLY=${PROJECT}.dll COPY --from=publish /release . COPY entrypoint.sh /bin/ COPY testconnection.sh /bin/ RUN chmod +x /bin/entrypoint.sh RUN chmod +x /bin/testconnection.sh ENTRYPOINT [ \u0026#34;/bin/entrypoint.sh\u0026#34; ] CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;dotnet $ASSEMBLY\u0026#34;] Subtle change; the location of the entrypoint.sh changed to /bin/entrypoint.sh and the $DB_SERVICE as well as the $DB_SERVICE_PORT arguments are missing. So, how does my entrypoint.sh know which server to check for connections before running our startup command? Thanks for asking:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash set -e # check if the startup command has been provided if [ \u0026#34;$1\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Startup command not set. Exiting\u0026#34; exit; fi # check if the $DB_SERVICE environment variable has been set if [ \u0026#34;$DB_SERVICE\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Environment variable \u0026#39;DB_SERVICE_NAME\u0026#39; not set. Exiting.\u0026#34; exit; fi # check if the $DB_SERVICE_PORT environment variable has been set if [ \u0026#34;$DB_SERVICE_PORT\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Environment variable \u0026#39;DB_SERVICE_PORT\u0026#39; not set. Exiting.\u0026#34; exit; fi echo \u0026#34;Testing connection to ${DB_SERVICE}:${DB_SERVICE_PORT}\u0026#34; until /bin/test-connection.sh $DB_SERVICE_NAME $DB_SERVICE_PORT; do \u0026gt;\u0026amp;2 echo \u0026#34;DB Service is starting up\u0026#34; sleep 1 done \u0026gt;\u0026amp;2 echo \u0026#34;DB Service is up - executing command: \u0026#39;$@\u0026#39;\u0026#34; # https://stackoverflow.com/a/3816747 exec \u0026#34;$@\u0026#34; exit 0 As you can see, I am reading environment variables in the bash script and I use all arguments to this script as the startup command. You may prefer it the other way, but I like it this way, and I hope you find this post valuable.\nAs always, all comments and feedback are greatly appreciated.\nCredits:\nHow to check if an environment variable exists and get its value?\n","date":"2021-03-23T18:08:25Z","image":"https://www.chingono.com/blog/2021/03/23/optimizing-dockerfile-startup-script-with-environment-variables/cover_hu79b5dba812486e751d6f5b1c2c232866_25286_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2021/03/23/optimizing-dockerfile-startup-script-with-environment-variables/","title":"Optimizing Dockerfile Startup Script With Environment Variables"},{"content":"In my previous post; Reference Build Arguments in Docker Startup Script, I showed how I added reusability to my Dockerfile by adding build arguments.\nThe docker builder CMD reference document states:\nIf you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. See ENTRYPOINT.\nIf the user specifies arguments to docker run then they will override the default specified in CMD.\nSo, after some fiddling around, I ended up with the following final version of my Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # [Choice] .NET Core version: 5.0, 3.1, 2.1 ARG VARIANT=3.1 ARG DB_SERVICE=db ARG DB_SERVICE_PORT=1433 ARG PROJECT=Business.Web # create a base runtime image with node FROM mcr.microsoft.com/dotnet/core/aspnet:${VARIANT} AS runtime EXPOSE 80 EXPOSE 443 RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # create a base SDK image with node FROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT} as sdk RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # copy source files FROM sdk as build COPY src/ ./src # restore nuget packages WORKDIR /src RUN dotnet restore # run publish command FROM build as publish RUN dotnet publish -c Release -o /release --no-restore # create release image from base runtime image FROM runtime AS release # https://github.com/moby/moby/issues/37345#issuecomment-400245466 ARG PROJECT ENV ASSEMBLY=${PROJECT}.dll COPY --from=publish /release . COPY entrypoint.sh . COPY testconnection.sh . RUN chmod +x ./entrypoint.sh RUN chmod +x ./testconnection.sh ENTRYPOINT [ \u0026#34;./entrypoint.sh\u0026#34; ] CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;$DB_SERVICE $DB_SERVICE_PORT dotnet $ASSEMBLY\u0026#34;] Configuring it this way allows me to supply different arguments when running the container without having to specify entrypoint.sh every time. Small change, and I hope this proves helpful to you, dear reader. All comments and feedback will be greatly appreciated.\n","date":"2021-03-15T17:51:18Z","image":"https://www.chingono.com/blog/2021/03/15/dockerfile-combine-entrypoint-cmd/cover_hubb1745ac1c8ec556e18776479db3c0c5_28145_120x120_fill_box_smart1_3.png","permalink":"https://www.chingono.com/blog/2021/03/15/dockerfile-combine-entrypoint-cmd/","title":"Combining ENTRYPOINT and CMD in a Dockerfile"},{"content":"In my previous post; Waiting for Docker Service Container Port to Be Ready, I showed how I managed to delay container application code execution until another service is ready to accept tcp connections.\nIf this post, I\u0026rsquo;m going to improve on that solution and make it more reusable by adding environment variables to the the Dockerfile and to the docker-compose.yml.\nHere\u0026rsquo;s the updated Dockerfile:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 # [Choice] .NET Core version: 5.0, 3.1, 2.1 ARG VARIANT=3.1 ARG DB_SERVICE=db ARG DB_SERVICE_PORT=1433 ARG PROJECT=Business.Web # create a base runtime image with node FROM mcr.microsoft.com/dotnet/core/aspnet:${VARIANT} AS runtime EXPOSE 80 EXPOSE 443 RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # create a base SDK image with node FROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT} as sdk RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # copy source files FROM sdk as build COPY src/ ./src # restore nuget packages WORKDIR /src RUN dotnet restore # run publish command FROM build as publish RUN dotnet publish -c Release -o /release --no-restore # create release image from base runtime image FROM runtime AS release # https://github.com/moby/moby/issues/37345#issuecomment-400245466 ARG PROJECT ENV DB_SERVICE=${DB_SERVICE} ENV DB_SERVICE_PORT=${DB_SERVICE_PORT} ENV ASSEMBLY=${PROJECT}.dll COPY --from=publish /release . COPY entrypoint.sh . COPY testconnection.sh . RUN chmod +x ./entrypoint.sh RUN chmod +x ./testconnection.sh CMD [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;./entrypoint.sh $DB_SERVICE $DB_SERVICE_PORT dotnet $ASSEMBLY\u0026#34;] And the docker-compose.yml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # https://docs.docker.com/compose/compose-file/compose-file-v3/ version: \u0026#39;3\u0026#39; services: app: build: context: . dockerfile: Dockerfile args: # [Choice] Update \u0026#39;VARIANT\u0026#39; to pick a .NET Core version: 2.1, 3.1, 5.0 VARIANT: 3.1 DB_SERVICE: db DB_SERVICE_PORT: 1433 PROJECT: Business.Web image: business_web environment: - ASPNETCORE_ENVIRONMENT=Testing - ASPNETCORE_URLS=http://+:80 - ASPNETCORE_ConnectionStrings__Db=Server=db;Database=Business.Web;User ID=sa;Password=V3ry$ecureP@ssw0rd;MultipleActiveResultSets=False;Connection Timeout=30; - DB_SERVICE=db - DB_SERVICE_PORT=1433 # https://docs.docker.com/compose/startup-order/ depends_on: - db restart: on-failure ports: - \u0026#34;8080:80\u0026#34; - \u0026#34;8443:443\u0026#34; volumes: - ~/.aspnet/https:/https:ro db: image: mcr.microsoft.com/mssql/server:2019-latest restart: unless-stopped environment: - SA_PASSWORD=V3ry$ecureP@ssw0rd - ACCEPT_EULA=Y With this setup, I can create multiple image variations with the same Dockerfile by supplying the three arguments:\n1 2 3 DB_SERVICE: db DB_SERVICE_PORT: 1433 PROJECT: Business.Web And when I run the docker image, I can also supply the same three environment variables in order to override the container defaults. Hopefully this proves helpful to you, dear reader. All feedback will be greatly appreciated.\nCredits:\nPersisting ENV and ARG settings to all later stages in multi-stage builds\n","date":"2021-03-11T21:27:40Z","image":"https://www.chingono.com/blog/2021/03/11/reference-environment-variables-docker-startup-script/cover_hu5baa1151b514a27cf3a27c15e1c673f5_55131_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2021/03/11/reference-environment-variables-docker-startup-script/","title":"Reference Environment Variables in Docker Startup Script"},{"content":"Recently, I was working on a pet project and had created my Dockerfile for the ASP.NET Core container as well as the docker-compose.yml file to compose the services.\nHere\u0026rsquo;s the Dockerfile I had:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # [Choice] .NET Core version: 5.0, 3.1, 2.1 ARG VARIANT=3.1 # create a base runtime image with node FROM mcr.microsoft.com/dotnet/core/aspnet:${VARIANT} AS runtime EXPOSE 80 EXPOSE 443 RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # create a base SDK image with node FROM mcr.microsoft.com/dotnet/core/sdk:${VARIANT} as sdk RUN curl -sL https://deb.nodesource.com/setup_10.x | bash - RUN apt-get install -y nodejs # copy source files FROM sdk as build COPY src/ ./src # install npm packages #WORKDIR /src/Business.Web/Spa #RUN npm install #RUN npm run build # restore nuget packages WORKDIR /src RUN dotnet restore # run publish command FROM build as publish RUN dotnet publish -c Release -o /release --no-restore # create release image from base runtime image FROM runtime AS release COPY --from=publish /release . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;Business.Web.dll\u0026#34;] And the docker-compose.yml file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # https://docs.docker.com/compose/compose-file/compose-file-v3/ version: \u0026#39;3\u0026#39; services: app: build: context: . dockerfile: Dockerfile args: # [Choice] Update \u0026#39;VARIANT\u0026#39; to pick a .NET Core version: 2.1, 3.1, 5.0 VARIANT: 3.1 image: business_web environment: - ASPNETCORE_ENVIRONMENT=Testing - ASPNETCORE_URLS=http://+:80 - ASPNETCORE_ConnectionStrings__Db=Server=db;Database=Business.Web;User ID=sa;Password=V3ry$ecureP@ssw0rd;MultipleActiveResultSets=False;Connection Timeout=30; # https://docs.docker.com/compose/startup-order/ depends_on: - db restart: on-failure ports: - \u0026#34;8080:80\u0026#34; - \u0026#34;8443:443\u0026#34; volumes: - ~/.aspnet/https:/https:ro db: image: mcr.microsoft.com/mssql/server:2019-latest restart: unless-stopped environment: - SA_PASSWORD=V3ry$ecureP@ssw0rd - ACCEPT_EULA=Y My biggest challenge was that even though I had set the app service to depend on the db service, the app service would start before the db service was really ready to receive connections. This resulted in .net startup errors, crash loops and the container eventually shutting down:\n1 2 3 4 5 6 7 8 9 info: Microsoft.EntityFrameworkCore.Infrastructure[10403] Entity Framework Core 3.1.3 initialized \u0026#39;DataContext\u0026#39; using provider \u0026#39;Microsoft.EntityFrameworkCore.SqlServer\u0026#39; with options: None crit: Microsoft.AspNetCore.Hosting.Diagnostics[6] Application startup exception Microsoft.Data.SqlClient.SqlException (0x80131904): A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: TCP Provider, error: 35 - An internal exception was caught) ---\u0026gt; System.Net.Internals.SocketExceptionFactory+ExtendedSocketException (00000001, 11): Resource temporarily unavailable Since there was some initialization code and database seeding that had to take place before the application was ready to run, I wanted to ensure that the dotnet code started only when the database container/service was ready to receive connections.\nAfter some searching on the internet, I found this answer on the Unix StackExchange:\nTesting remote TCP port using telnet by running a one-line command\nNow, I needed to ensure this command runs before launching the .net site, and only after the db server was ready to receive connections on port 1433. So, after some further searching, trial and error, I eventually landed on the following two scripts:\ntestconnection.sh is a copy of the code found on Unix StackExchange:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/bash # https://unix.stackexchange.com/a/406356 if [ \u0026#34;$2\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Syntax: $0 \u0026lt;host\u0026gt; \u0026lt;port\u0026gt;\u0026#34; exit; fi host=$1 port=$2 r=$(bash -c \u0026#39;exec 3\u0026lt;\u0026gt; /dev/tcp/\u0026#39;$host\u0026#39;/\u0026#39;$port\u0026#39;;echo $?\u0026#39; 2\u0026gt;/dev/null) if [ \u0026#34;$r\u0026#34; = \u0026#34;0\u0026#34; ]; then echo \u0026#34;$host $port is open\u0026#34; else echo \u0026#34;$host $port is closed\u0026#34; exit 1 # To force fail result in ShellScript fi entrypoint.sh is the entry point of the container. It checks the tcp port for readiness then executes the command for the container when the tcp port is open:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #!/bin/bash set -e # get the first two arguments server=$1 port=$2 # check if we have 3 or more arguments if [ \u0026#34;$3\u0026#34; == \u0026#34;\u0026#34; ]; then echo \u0026#34;Syntax: $0 \u0026lt;host\u0026gt; \u0026lt;port\u0026gt; \u0026lt;command\u0026gt; [\u0026lt;arg\u0026gt;, \u0026lt;arg\u0026gt;, ...]\u0026#34; exit; fi # use the first two arguments to test the tcp connection echo \u0026#34;Testing connection to ${server}:${port}\u0026#34; until ./testconnection.sh $server $port; do \u0026gt;\u0026amp;2 echo \u0026#34;SQL Server is starting up\u0026#34; sleep 1 done \u0026gt;\u0026amp;2 echo \u0026#34;SQL Server is up - executing command\u0026#34; # https://stackoverflow.com/a/3816747 # use the rest of the arguments to start up the container exec \u0026#34;${@:3}\u0026#34; Key points to note here are that entrypoint.sh uses the first two arguments for checking the tcp port and the rest of the arguments for container startup. So, instead of starting up our container this way:\n1 ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;Business.Web.dll\u0026#34;] We will use the following approach instead:\n1 2 3 4 5 6 COPY entrypoint.sh . COPY testconnection.sh . RUN chmod +x ./entrypoint.sh RUN chmod +x ./testconnection.sh CMD /bin/bash ./entrypoint.sh db 1433 dotnet Business.Web.dll When the services are started with this new dockerfile, this is what I got in the logs:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 SQL Server is starting up SQL Server is starting up SQL Server is starting up SQL Server is starting up SQL Server is starting up SQL Server is up - executing command: \u0026#39;sh -c dotnet Business.Web.dll\u0026#39; Testing connection to data:1433 data 1433 is closed data 1433 is closed data 1433 is closed data 1433 is closed data 1433 is closed data 1433 is open info: Microsoft.AspNetCore.DataProtection.KeyManagement.XmlKeyManager[64] Azure Web Sites environment detected. Using \u0026#39;/root/ASP.NET/DataProtection-Keys\u0026#39; as key repository; keys will not be encrypted at rest. warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60] Storing keys in a directory \u0026#39;/root/ASP.NET/DataProtection-Keys\u0026#39; that may not be persisted outside of the container. Protected data will be unavailable when info: Microsoft.EntityFrameworkCore.Infrastructure[10403] Entity Framework Core 5.0.6 initialized \u0026#39;DataContext\u0026#39; using provider \u0026#39;Microsoft.EntityFrameworkCore.SqlServer\u0026#39; with options: None info: Microsoft.EntityFrameworkCore.Database.Command[20101] Executed DbCommand (22ms) [Parameters=[], CommandType=\u0026#39;Text\u0026#39;, CommandTimeout=\u0026#39;30\u0026#39;] SELECT 1 As you can see, this solution allowed my app container to wait for as long as it needed to before starting the .net application. Hopefully, you find that helpful, dear reader.\nCredits:\nTesting remote TCP port using telnet by running a one-line command\nHow to pass all arguments passed to my bash script to a function of mine?\n","date":"2021-03-04T12:21:28Z","image":"https://www.chingono.com/blog/2021/03/04/waiting-docker-service-container-port-ready/cover_hu62f23ae666e781c65386d17f3dc1416f_38807_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.chingono.com/blog/2021/03/04/waiting-docker-service-container-port-ready/","title":"Waiting for Docker Service Container Port to Be Ready"}]